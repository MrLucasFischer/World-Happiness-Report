---
title: "World Happiness Report"
subtitle: "Estatistica Multivariada - Professora Regina Bispo"
author: "Lucas Santos Fischer"
date: "dezembro, 2018"
documentclass: "report"
geometry: margin=1.5cm
fontsize: 12pt
header-includes:
    - \usepackage{setspace}\onehalfspacing
    - \usepackage{float}
    - \usepackage{booktabs}
    - \usepackage{longtable}
    - \usepackage{floatrow}
    - \floatsetup[table]{capposition=bottom}
output: 
  pdf_document:
    toc: true
    toc_depth: 4
    latex_engine: xelatex
    number_sections: false
    fig_caption: true
    includes:
      in_header: header.tex
linkcolor: "blue"
mainfont: Times New Roman
bibliography: biblio.bib
biblio-style: "apalike"
link-citations: true
---

\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introdução

No ambito de estudar um _dataset_ foi escolhido o \href{https://www.kaggle.com/unsdsn/world-happiness}{World Happiness Report}. Este é um _dataset_ com informação sobre o _ranking_ de felicidade de cada país, e as variáveis que contribuem para o mesmo, durante um periodo de três anos (2015, 2016 e 2017). Para além da informação sobre o __nome__ e __região__ de um determinado país, estes três ficheiros possuem as seguintes variáveis que explicam a importância de um certo fator para a posição de um país no _rank_ de felicidade:

* __GDP__ - A importância do produto interno bruto de um país para o _rank_
* __Family__ - A importância da família para o _rank_
* __Life Expectancy__ - A importância da esperança média de vida de um país para o _rank_
* __Freedom__ - A importância da liberdade de um país para o _rank_
* __Government Corruption__ - A importância da corrupção do governo de um país para o _rank_
* __Genorosity__ - A importância de genorosidade de um país para o _rank_

Com estas informações existem várias perguntas interessantes que serão respondidas com este trabalho, nomeadamente: __Serão os dados relativamente iguais entre os três anos?__ ; __Quais serão as variáveis que mais contribuem para a felicidade de um país?__ ; __Como se distinguem os diferentes continentes no que toca a sua felicidade?__ Neste trabalho serão respondidas estas perguntas bem como demonstrado todos os passos necessários até obter as suas respostas.

## Métodos

Durante o estudo deste _dataset_ foram utilizados diferentes metodos que serão resumidos neste capítulo. De maneira a ser possível responder a todas as perguntas propostas foram utilizados os métodos de __Análise de Perfis__, __Teste de Mauchly__, __Análise de componentes principais__ e __Análise Discriminante__.

### Análise de perfis

A primeira grande pergunta com interesse em responder é: __Serão os dados significativamente iguais entre os três anos?__ Neste capítulo será descrito o processo feito de maneira a obter a sua resposta.
Visto que o estudo é realizado sobre amostras independentes, uma vez que não existe ligação entre um _dataset_ e outro, i.e. não existe ligação entre os diferentes anos de medição, o processo estatístico para obter a resposta a esta pergunta passa pela __Análise de Perfis__. Neste processo existem três questões com particular interesse:

* Serão os perfis paralelos?
* Serão os perfis coincidentes (dado que são paralelos)?
* Serão os perfis horizontais (dado que são paralelos e coincidentes)?

#### Teste ao paralelismo dos perfis
A primeira questão a ser respondida deverá sempre ser quanto ao paralelismo dos perfis uma vez que esta constitui uma condição de modo aos seguintes estudos serem possíveis.
Este teste ao paralelismo pode expressar-se pela hipótese:
\begin{equation}
  \label{eq:h0_paralel}
  H_0 : C\mu_1 - C\mu_2 = 0
\end{equation}

Onde em \ref{eq:h0_paralel} __C__ representa a matriz de contrastes dos perfis que neste estudo equivale à matriz:

\begin{equation}
  C = 
  \left[ {\begin{array}{cccccc}
   -1 & 1 & 0 & 0 & 0 & 0 \\
    0 & -1 & 1 & 0 & 0 & 0 \\
    0 & 0 & -1 & 1 & 0 & 0 \\
    0 & 0 & 0 & -1 & 1 & 0 \\
    0 & 0 & 0 & 0 & -1 & 1 \\
  \end{array} } \right]
\end{equation}

Sob $H0$ a estatistica teste é:

\begin{equation}
  \label{eq:valor_obs_paralel}
  T^2 = (C(\bar{x}_1 - \bar{x}_2))'\left[\left(\frac{1}{n_1} + \frac{1}{n_2}\right)CS_{pooled}C'\right]^{-1}(C(\bar{x}_1 - \bar{x}_2))\sim T^2_{q-1}(n_1 + n_2 - 2)
\end{equation}

e o quantil critico:
\begin{equation}
  \label{eq:quantil_crit_paralel}
  \frac{(n_1 + n_2 - 2)(q - 1)}{n_1 + n_2 - q}F_{(q - 1, n_1 + n_2 - q); 1 - \alpha}
\end{equation}

Este teste permite-nos concluir se a variância entre os pontos observados nos _datasets_ é igual independentemente dos perfis (anos dos _datasets_).

#### Teste à coincidência dos perfis

Caso seja provado o paralelismo entre os dois perfis pode-se estudar a sua coinciência. No teste da coincidência de perfis procura-se determinar se os valores médios de cada ponto observados nos dois perfis são os mesmo, dado que já se comprovou a igualdade na variância dos pontos. Este teste é expressado pela hipotese descrita [@slides_EM] em \ref{eq:h0_coincident}

\begin{equation}
  \label{eq:h0_coincident}
  H_0 : 1'\mu_1 - 1'\mu_2 = 0
\end{equation}

Onde a estatistica teste sob $H0$ é igual à expressão descrita em 
\begin{equation}
  \label{eq:valor_obs_coincident}
  T^2 = (1(\bar{x}_1 - \bar{x}_2))'\left[\left(\frac{1}{n_1} + \frac{1}{n_2}\right)1'S_{pooled}1\right]^{-1}(1'(\bar{x}_1 - \bar{x}_2))
\end{equation}

e o seu quantil critico:

\begin{equation}
  \label{eq:quantil_crit_coincident}
  F_{(1, n_1 + n_2 - q); 1 - \alpha}
\end{equation}

#### Teste à horizontalidade dos perfis

A ultima questão relevante a responder na análise de perfis relaciona-se com a horizontalidade dos perfis, i.e. testar se as médias são iguais independentemente das variáveis. Uma vez verificada o paralelismo e coincidência entre dois perfis faz sentido estudar quanto a sua horizontalidade. Este teste pode ser expressado pela hipótese descrita em \ref{eq:h0_horizontal} [@sebenta_EM].

\begin{equation}
  \label{eq:h0_horizontal}
  H_0 : \frac{C(\mu_1 + \mu_2)}{2} = 0
\end{equation}

Sob $H_0$ advém a estatistica teste

\begin{equation}
  \label{eq:et_horizontal}
  T^2 = (n_1 + n_2)(C\bar{x})'(CS_{pooled}C')^{-1}C\bar{x}
\end{equation}

e o quantil critico que é idêntico ao quantil critico descrito em \ref{eq:quantil_crit_paralel}.

### Teste de Mauchly 

De modo testarmos a adequabilidade de usarmos a técnica da análise de componentes principais precisamos de primeiro verificar se existem variáveis correlacionadas/redundantes de modo a fazer sentido a aplicação desta técnica. O teste de Mauchly pretende testar isto mesmo, testando se a matriz de correlações é significativamente diferente da matriz idêntidade através da hipótese \ref{eq:h0_mauchly}

\begin{equation}
  \label{eq:h0_mauchly}
  H_0 : \Sigma = \sigma^2I
\end{equation}

Sob $H_0$ temos a estatistica teste \ref{eq:et_mauchly}

\begin{equation}
  \label{eq:et_mauchly}
  U^* = -\left(n - 1 - \frac{2p^2 + p + 2}{6p}\right)ln U
\end{equation}

com $U = \frac{p^p\det{S}}{(tr(S))^p}$

e com o quantil critico descrito por \ref{eq:qt_mauchly}

\begin{equation}
  \label{eq:qt_mauchly}
  \chi^2_{\frac{1}{2}p(p + 1) - 1; 1 - \alpha}
\end{equation}

No entanto o teste de esfericidade de Mauchly pressupõe normalidade multivariada, por isso é necessário primeiro verificarmos esta propriedade sobre os nossos dados, algo que conseguimos fazer através de um __gráfico QQ__. Este gráfico relaciona os quantis amostrais (empiricos) retirados dos nossos _datasets_ e relaciona-os com os quantis teóricos que seriam obtidos caso os dados sejam provenientes de uma distribuição normal multivariada.

\begin{figure}[!ht]
\centering
\caption{QQ \emph{plot} do \emph{dataset} de 2015 de modo a verificar o seu ajustamento a uma normal multivariada}
\includegraphics[width=0.8\textwidth]{images/qqplot_2015.png}
\label{fig:qqplot}
\end{figure}

Observando o gráfico \ref{fig:qqplot} conseguimos observar que os pontos não se afastam das linhas tracejadas, isto indica que o _dataset_ está bem ajustado a uma normal multivariada (algo que se verifica para os restas dois _datasets_) então podemos executar o teste de esfericidade de Mauchly.

### Análise de componentes principais

A resposta às questões: __Quais serão as variáveis que mais contribuem para a felicidade de um país?__ e __Poderemos representar os dados de uma maneira mais resumida?__ são obtidas através de uma técnica conhecido como Análise de componentes principais. O objetivo desta técnica é a __redução da dimensionalidade__ dos dados, mantendo a maior percentagem de variabilidade possível, e a __transformação de variáveis correlacionadas em variáveis independentes__. Esta redução da dimensionalidade consiste em encontrar uma projeção em menores dimensões que permita preservar a maior variabilidade possível dos dados. Esta projeção é encontrada sobre os vetores próprios da matriz variância-covariância __S__ então as K componentes principais são definidas por:

\begin{equation}
  y = a_{k1}x_1 + a_{k2}x_2 + ... + a_{kp}x_p
\end{equation}

onde $a_k$ representa o k-ésimo vetor próprio de __S__.

Uma vez obtidas as componentes principais é necessário escolher quais as que serão retidas. Esta decisão pode ser feita segundo diversos critérios :

* Percentagem de variabilidade explicada - Neste critério podemos estabelecer uma percentagem da variabilidade explicada que queremos reter e guardar as componentes principais suficientes para atingir essa percentagem

* Critério de Kaiser - Reter apenas as variáveis cujo valor próprio seja superior a 1 (caso as variáveis estejam padronizadas)

* Scree plot - Metodo mais gráfico que determina o número de componentes principais pelo ponto onde a linha desenhada diminui acentuadamente de inclinação tornando-se horizontal

### Análise Discriminante

De maneira a se responder à ultima questão: __Como se distinguem os diferentes continentes no que toca a sua felicidade?__ é necessário identificar um discriminante entre os vários continentes. Este discriminante é obtido através da análise discriminante. O objetivo desta técnica é discriminar grupos a partir de informação recolhida sobre os elmentos que constituem esses grupos, em termos práticos esta técnica coincide com determinar as combinações lineares de variáveis que maximizam a diferença entre os grupos (*discriminação*). Com base nestas combinações lineares é possível posteriormente prever a pertença de um ponto não agrupado a um certo grupo (*classificação*).

Visto neste caso existirem mais que dois grupos (6 continentes) os coeficientes das combinações lineares são obtidas maximizando a razão \ref{eq:coefs_disc}

\begin{equation}
\label{eq:coefs_disc}
  \frac{a'Ba}{a'Wa}
\end{equation}

Então os coeficientes das combinações lineares que melhor descriminam os grupos equivalem assim aos coeficientes dos vetores próprios da matriz $W^{-1}B$

\begin{equation}
\label{eq:lin_disc}
  y_i = a'_ix (i = 1, ..., s)
\end{equation}


## Resultados

Neste capítulo serão apresentados os resultados obtidos para os diferentes metodos de estudo a cima descritos, bem como uma interpretação aos mesmos de modo a conseguir-se responder às questões propostas para este _dataset_. De maneira a ser possível estudar o _dataset_ é necessário primeiro processar os dados de maneira a facilitar a sua utilização, este pre-processamento encontra-se no ficheiro `world_happiness.R`.

```{r ler_dados, include = FALSE, echo=FALSE}
#Leitura dos dados
dados_2015 = as.data.frame(read.csv("./Dataset/2015.csv", header = TRUE),
                           stringsAsFactors = FALSE)

dados_2016 = as.data.frame(read.csv("./Dataset/2016.csv", header = TRUE),
                           stringsAsFactors = FALSE)

dados_2017 = as.data.frame(read.csv("./Dataset/2017.csv", header = TRUE),
                           stringsAsFactors = FALSE)

#Renomear "Taiwan Province of China" to "Taiwan"
levels = levels(dados_2017$Country)
levels[length(levels) + 1] = "Taiwan"
dados_2017[33, 1] = NA

dados_2017$Country = factor(dados_2017$Country, levels = levels)
dados_2017$Country[is.na(dados_2017$Country)] = "Taiwan"

#Renomear "Hong Kong S.A.R, China" to "Hong Kong"
levels = levels(dados_2017$Country)
levels[length(levels) + 1] = "Hong Kong"
dados_2017[71, 1] = NA

dados_2017$Country = factor(dados_2017$Country, levels = levels)
dados_2017$Country[is.na(dados_2017$Country)] = "Hong Kong"
```

```{r coluna_regiao, include = FALSE, echo=FALSE}
#Visto que os dados de 2017 não tem a coluna de Region vamos adiciona-la com base nos outros datasets
region_column = c()
for (row in 1:nrow(dados_2017) ) {
  
  country_name = as.character(dados_2017[row, 1]) #Obtem o nome do país presenta na linha "row"
  
  region_name_2015 = as.character(dados_2015[dados_2015[ , 1] == country_name  , 2])
  
  if(length(region_name_2015) == 0) { #Se o dataset de 2015 não tiver o país country_name então tenta-se com o dataset de 2016
    
    region_name_2016 = as.character(dados_2016[dados_2016[ , 1] == country_name  , 2]) 
    
    if(length(region_name_2016) == 0) {
      
      region_column[row] = NA #Se o dataset de 2016 também não tiver o país country_name por NA
      
    } else {
      
      region_column[row] = region_name_2016
      
    }
    
  } else {
    region_column[row] = region_name_2015
  }
  
}

dados_2017$Region = region_column #Adicionar uma coluna chamada Region ao dataset de 2017

#Simplificar o nome dos dados
colnames(dados_2015) = c("Country", "Region", "Rank", "Score", "Error", "GDP", "Family", "Life Expectancy", "Freedom", "Government Corruption", "Generosity", "Dystopia")
colnames(dados_2016) = c("Country", "Region", "Rank", "Score", "Lower Confidence", "Upper Confidence", "GDP", "Family", "Life Expectancy", "Freedom", "Government Corruption", "Generosity", "Dystopia")
colnames(dados_2017) = c("Country", "Rank", "Score", "Upper Confidence", "Lower Confidence","GDP", "Family", "Life Expectancy", "Freedom", "Generosity", "Government Corruption", "Dystopia", "Region")

dados_2017 = dados_2017[, c(1, 13, 2, 3, 4, 5, 6, 7, 8, 9, 11, 10, 12)] #Mudar a posição da ultima coluna para ser igual à de 2015 e 2016 e meter a Region como segunda coluna
```

```{r padronizar_dados, include = FALSE, echo=FALSE}
#Padronizar os dados
dados_2015[, -c(1:5)] = scale(dados_2015[, -c(1:5)], scale = TRUE, center = TRUE)
dados_2016[, -c(1:6)] = scale(dados_2016[, -c(1:6)], scale = TRUE, center = TRUE)
dados_2017[, -c(1:5)] = scale(dados_2017[, -c(1:5)], scale = TRUE, center = TRUE)

#Ficar apenas com as variaveis de interesse para a PCA
dados_2015_var = dados_2015[, -c(c(1:5), 12)]
dados_2016_var = dados_2016[, -c(c(1:6), 13)]
dados_2017_var = dados_2017[, -c(c(1:6), 13)]
```

### Análise preliminar dos dados
A análise preliminar dos dados (ou _Exploratory data analysis_) constitui uma boa prática para iniciar o estudo de qualquer _dataset_. Esta análise tem como objetivo obter uma familiarização dos dados de maneira a ser possível obter uma maior sensibilidade quanto ao estudo dos mesmos.

Nesta pequena análise introdutória são normalmente verificadas as médias e variâncias dos dados bem como alguns gráficos que permitam visualizar estes resultados e também outros como a correlação entre os dados.

```{r analise_preliminar_1, include=FALSE, echo=FALSE}

#Obter a média dos datasets
xbar_1 = colMeans(dados_2015_var)
xbar_2 = colMeans(dados_2016_var)
xbar_3 = colMeans(dados_2017_var)

#Numero de variaveis nos dados, é identico em todos os datasets por isso basta ver para um
p = ncol(dados_2015_var)
correlacoes_distintas = p * (p-1) / 2

#Ver correlações entre as variáveis
s_2015 = var(dados_2015_var)
s_2016 = var(dados_2016_var)
s_2017 = var(dados_2017_var)

#round(s_2015, 3)

#Vemos que para os datasets de 2016 e 2017 os valores são mais ou menos idênticos
#round(s_2016, 3)
#round(s_2017, 3)
```

```{r pais_plot, echo=TRUE, message=FALSE, warning=FALSE, fig.show = 'hide'}
pairs(dados_2015_var)
#pairs(dados_2016_var)
#pairs(dados_2017_var)
```

\begin{figure}[ht]
\centering
\caption{Representação gráfica da correlação entre pares de variáveis para o \emph{dataset} de 2015}
\includegraphics[width=0.8\textwidth]{images/pair_plot.png}
\label{fig:pairs}
\end{figure}


Fazendo o _plot_ das correlações entre pares de variáveis através da função `pairs` conseguimos obter uma representação gráfica da correlação entre as variáveis presente na figura \ref{fig:pairs}, especialmente entre as variáveis  __GDP__ e as variáveis __Family__ e __Life Expectancy__.

Com esta análise introdutória ficamos assim sensibilizados à correlação entre as variáveis e à possível existência de variáveis redundantes, algo que será estudado com mais detalhe no capítulo __Teste de Mauchly__ e __Análise de componentes principais__.


### Análise de perfis

```{r paralel_2015_2016, include=FALSE, echo=FALSE}
#Testar o paralelismo entre 2015 e 2016

contrastes = matrix(
  c(
    -1, 1, 0, 0, 0, 0,
    0, -1, 1, 0, 0, 0,
    0, 0, -1, 1, 0, 0,
    0, 0, 0, -1, 1, 0,
    0, 0, 0, 0, -1, 1
  ), 
  nrow = 5, ncol = 6, byrow = T
)

teste_paralelismo = function(dados_1, dados_2, c, alpha = 0.05) {
  #Executa o teste ao paralelismo sobre os dados enviados por parametros da função
  #Esta função retorna TRUE caso H0 não seja rejeitada, e FALSE caso contrário
  # c representa a matriz de contrastes
  #H0: Cmu1 = Cmu2
  
  n1 = nrow(dados_1)
  n2 = nrow(dados_2)
  q = ncol(dados_1) # = ncol(dados2) = ncol(contrastes)
  
  xbar_1 = colMeans(dados_1)
  xbar_2 = colMeans(dados_2)
  s_1 = var(dados_1)
  s_2 = var(dados_2)
  s_pooled = ((n1 - 1) / (n1 + n2 - 2)) * s_1 + ((n2 - 1) / (n1 + n2 - 2)) * s_2
  
  t_squared = t(c %*% (xbar_1 - xbar_2)) %*%
    solve(((1/n1) + (1/n2)) * c %*% s_pooled %*% t(c)) %*% (c %*% (xbar_1 - xbar_2))
  
  quantil_critico = ((n1 + n2 - 2) * (q - 1) / (n1 + n2 - q)) *
    qf(1 - alpha, q - 1, n1 + n2 - q)
  
  p_value = 1 - pf(t_squared / ((n1 + n2 - 2) *
                                  (q - 1) / (n1 + n2 - q)), q - 1, n1 + n2 - q)
  
  #Não se rejeita H0 se t_squared < quantil_critico e p_value > alpha
  return(t_squared < quantil_critico && p_value > alpha)
}

teste_coincidencia = function(dados_1, dados_2, alpha = 0.05) {
  #Executa o teste à coincidencia sobre os dados enviados por parametros da função
  #Esta função retorna TRUE caso H0 não seja rejeitada, e FALSE caso contrário
  #H0: 1'mu1 = 1'mu2
  
  ones = rep(1, ncol(dados_1))
  n1 = nrow(dados_1)
  n2 = nrow(dados_2)
  
  xbar_1 = colMeans(dados_1)
  xbar_2 = colMeans(dados_2)
  s_1 = var(dados_1)
  s_2 = var(dados_2)
  s_pooled = ((n1 - 1) / (n1 + n2 - 2)) * s_1 + ((n2 - 1) / (n1 + n2 - 2)) * s_2
  
  t_squared = t(t(ones) %*% (xbar_1 - xbar_2)) %*% solve(((1/n1) + (1/n2)) * t(ones) %*% s_pooled %*% ones) %*% (t(ones) %*% (xbar_1 - xbar_2)) #valor observado
  
  quantil_critico = qf(1 - alpha, 1 , n1 + n2 -2)
  
  p_value = 1 - pf(t_squared, 1 , n1 + n2 -2)
  
  #Não se rejeita H0 se t_squared < quantil_critico e p_value > alpha
  return(t_squared < quantil_critico && p_value > alpha)
}

teste_horizontalidade = function(dados_1, dados_2, c, alpha = 0.05) {
  #Executa o teste à horizontalidade sobre os dados enviados por parametros da função
  #Esta função retorna TRUE caso H0 não seja rejeitada, e FALSE caso contrário
  # c representa a matriz de contrastes
  #H0: 1/2 * C * (mu1 + mu 2) = 0
  
  n1 = nrow(dados_1)
  n2 = nrow(dados_2)
  q = ncol(dados_1) # = ncol(dados2) = ncol(contrastes)
  
  xbar_1 = colMeans(dados_1)
  xbar_2 = colMeans(dados_2)
  xbar = (n1 * xbar_1 + n2 * xbar_2) / (n1 + n2)
  
  s_1 = var(dados_1)
  s_2 = var(dados_2)
  s_pooled = ((n1 - 1) / (n1 + n2 - 2)) * s_1 + ((n2 - 1) / (n1 + n2 - 2)) * s_2
  
  t_squared = (n1 + n2) * t(c %*% xbar) %*%
    solve(c %*% s_pooled %*% t(c)) %*% (c %*% xbar)
  
  quantil_critico = ((n1 + n2 - 1) * (q -1) / (n1 + n2 - q)) *
    qf(1 - alpha, q - 1, n1 + n2 - q)
  
  p_value = 1 - pf(t_squared / ((n1 + n2 - 1) *
                                  (q -1) / (n1 + n2 - q)), q - 1, n1 + n2 - q)
  
  #Não se rejeita H0 se t_squared < quantil_critico e p_value > alpha
  return(t_squared < quantil_critico && p_value > alpha)
}
```

Durante o estudo da análise de perfis foi primeiro executado os testes de paralelismo, coincidência e horizontalidade sobre os perfis de 2015 e 2016. O primeiro teste executado foi o teste ao paralelismo sobre estes dois perfis visto que o paralelismo entre perfis é uma condição para a sua coincidência.

```{r paralel_test, include=TRUE, echo=TRUE}
#Retorna TRUE caso H0 não seja rejeitada
teste_paralelismo(dados_2015_var, dados_2016_var, contrastes)
```
Não existindo evidências estatisticas para rejeitar $H_0$ conclui-se então que os dois perfis são paralelos, i.e. a variância entre os pontos presentes em ambos os _datasets_ é independente dos perfis. Tendo verificado o paralelismo entre estes dois perfis faz sentido estudar quanto a sua coincidência.

```{r coincident_test, include=TRUE, echo=TRUE}
#Retorna TRUE caso H0 não seja rejeitada
teste_coincidencia(dados_2015_var, dados_2016_var)
```
Da execução deste teste podemos observar que não existem evidências estatisticas para rejeitar $H_0$ então conclui-se que estes dois perfis são coincidentes, logo com esta conclusão já podemos admitir que os perfis são exatamente iguais. O restante teste para executar sendo ele o teste à horizontalidade dos perfis pretende determinar se as médias são iguais independentemente das variãveis dos _datasets_

```{r horizontal_test, include=TRUE, echo=TRUE}
#Retorna TRUE caso H0 não seja rejeitada
teste_horizontalidade(dados_2015_var, dados_2016_var, contrastes)
```
Mais uma vez através da execução das instruções a cima descritas obtemos a conclusão de que não existem evidências estatisticas para rejeitar $H_0$, logo podemos admitir que os dois perfis são __paralelos__, __coincidentes__ e __horizontais__.

Uma vez comprovado o paralelismo, coincidência e horizontalidade dos dois perfis podemos utilizar um dos dois perfis para executarmos os mesmos testes mas agora contrastando com o perfil do _dataset_ de 2017, onde é possível obter as mesmas conclusões que as a cima descritas, sendo assim possível concluir que os três _datasets_ (2015, 2016 e 2017) são __paralelos__, __coincidentes__ e __horizontais__, i.e. os dados são significativamente iguais entre os três _datasets_.

```{r testes_2017, include=TRUE, echo=TRUE}

teste_paralelismo(dados_2016_var, dados_2017_var, contrastes) &&
  teste_coincidencia(dados_2016_var, dados_2017_var) &&
  teste_horizontalidade(dados_2016_var, dados_2017_var, contrastes)

```


### Teste de Mauchly

```{r mauchly_test, include=FALSE, echo=FALSE}
mauchly_test = function(dados, alpha = 0.05) {
  #Executa o teste de Mauchly para os dados que forem passados por argumento
  #Esta função retorna TRUE caso H0 não seja rejeitada, e FALSE caso contrário
  
  library("psych")
  s = var(dados)
  n = nrow(dados)
  p = ncol(dados)
  
  u = ((p^p) * det(s)) / (tr(s))^p
  u_star = -(n - 1 - (((2 * p^2) + p + 2) / (6*p))) * log(u) #Valor observado
  p_value = 1 - pchisq(u_star, (1/2) * p * (p + 1) - 1) #P-value do valor observado
  
  quantil_critico = qchisq(1 - alpha, (1/2) * p * (p + 1) - 1) #Quantil critico
  
  #Não se rejeita H0 se u_star < quantil_critico e p_value > alpha
  return(u_star < quantil_critico && p_value > alpha)
}
```
Para a execução do teste de esfericidade de Mauchly, de modo a determinar a adequabilidade do uso da análise de componentes principais, foi implementada uma função no ficheiro `world_happiness.R` que executa a o teste de esfericidade de Mauchly para o _dataset_ enviado como argumento da função.

```{r mauchly_teste, include=TRUE, echo=TRUE}
mauchly_test(dados_2015_var) && mauchly_test(dados_2016_var) &&
  mauchly_test(dados_2017_var)
```
Verificando que para os três _datasets_ existem evidências estatisticas para rejeitar $H_0$ conclui-se então que para os três _datasets_ a matriz de correlações é significativamente diferente da matriz idêntidade o que significa que existem variâveis cuja sua correlação é significativamente diferente de 0 justificando assim a adequabilidade do uso da análise de componentes principais

O R disponibiliza também uma função que permite executar o teste de esfericidade de mauchly
```{r mauchly_test_r, include=TRUE, echo=TRUE, eval=FALSE}
mauchly.test(lm(as.matrix(dados_2015_var)~1))
mauchly.test(lm(as.matrix(dados_2016_var)~1))
mauchly.test(lm(as.matrix(dados_2017_var)~1))
```
Onde, observando os _p-values_ obtidos, se tiram as mesmas conclusões que a cima descritas.

### Análise de componentes principais

```{r mypca, include=FALSE, echo=FALSE}
pca = function(s, main_title) {
  #Realiza o estudo de componentes principais para
  #um determinado dataset precisando apenas 
  #de fornecer a matriz de variancias-covariancias
  #Esta função retorna os eigen vectors da matriz passada
  #por argumento da função que correspondem às componentes principais do dataset
  
  ev = eigen(s)
  variabilidade_total = sum(ev$values)
  
  prop_table = data.frame(ev = ev$values)
  prop_table$std = sqrt(prop_table$ev)
  prop_table$prop = prop_table$ev/6
  prop_table$cprop = cumsum(prop_table$prop)
  print(round(prop_table, 3))
  
  # plot(1 : nrow(prop_table), prop_table$ev,
  #      type = "b", main = main_title, ylab = "Valores proprios", xlab = "CP")
  # 
  # text(x = prop_table$ev,
  #      labels = paste(round(prop_table$ev, 2)), pos = 4, cex = 0.8)
  return(ev$vectors)
}
```

Tendo sido justificada a adequabilidade do uso do método da análise de componentes principais, seguiu-se então à implementação de uma função no ficheiro `world_happiness.R` de modo a executar este método.

```{r pca_result, echo = T, results = 'hide'}
componentes_principais_2015 = pca(s_2015, "Screeplot 2015 dataset")
```

\begin{table}[ht]
\centering
\label{tab:pca_result}
\begin{tabular}{cccc}
ev & std & prop & cprop \\ \hline
2.877 & 1.696 & 0.479 & 0.479 \\
1.325 & 1.151 & 0.221 & 0.700 \\
0.703 & 0.839 & 0.117 & 0.817 \\
0.560 & 0.748 & 0.093 & 0.911 \\
0.385 & 0.621 & 0.064 & 0.975 \\
0.150 & 0.387 & 0.025 & 1.000 \\
\end{tabular}
\caption{Resultado da análise de componentes principais para o \emph{dataset} de 2015}
\end{table}

\begin{figure}[!ht]
\centering
\caption{Scree plot obtido através das componentes principais do \emph{dataset} de 2015}
\includegraphics[width=0.8\textwidth]{images/scree_plot.png}
\label{fig:screeplot}
\end{figure}


A execução desta função escreve para a consola uma tabela com informação sobre os valores próprios da matriz de variância-covariância fornecida, bem como a variabilidade explicada por cada componente principal e também a vairabilidade cumulativa até a componente principal K. Esta função também esboça o _Scree plot_ \ref{fig:screeplot} e finalmente retorna todas as componentes principais obtidas.
Analisando a tabela escrita por esta função e seguindo o critério da percentagem de variabilidade explicada, querendo manter uma variabilidade de 80% seria necessário reter as três primeiras componentes principais, visto que só com a partir da terceira componente principal se consegue obter uma variabilidade cumulativa de 81.7%. Seguindo agora o método da análise do _Scree plot_ pode-se observar que é um metodo muito mais subjetivo devido à dificuldade em analisar com precisão o ponto onde o declive do gráfico passa a ser horizontal, mas é possível identificar que seria em torno da terceira componente principal, escolhendo assim três componentes principais. Por fim temos o critério de Kaise, um critério mais objetivo, que foi o critério utilizado durante este estudo. Observando a tabela escrita pela execução da função `pca` verifica-se que seriam retidas apenas as primeiras duas componentes principais visto serem as únicas cujo valor próprio é superior a 1. Este critério baseia-se no facto de que se as variáveis não possuem a capacidade de explicar a sua própria variabilidade (o que acontece quando o seu valor próprio é inferior a 1) então não têm grande interesse em serem retidas no _dataset_.

O R possui também uma função que permite estudar a análise de componentes principais de uma maneira mais simples através da função `prcomp` e também uma função `screeplot` que premite esboçar o _Scree plot_

```{r prcomp, include=TRUE, echo=TRUE, eval=FALSE}
pca_2015 = prcomp(dados_2015_var)
summary(pca_2015)
screeplot(pca_2015, type = "l", main = "Screeplot 2015 dataset")
```

Onde, usando os mesmos critérios que acima descritos, obtemos exatamente as mesmas conclusões.

O mesmo procedimento foi feito para os restantes dois _datasets_ obtendo resultados semelhantes, então é possível identificar que a primeira e segunda componente principal de cada _dataset_ é suficiente para representar cerca de 70% da variabilidade total dos _datasets_ podendo-se utilizar as mesmas como substitutas à utilização de todas as variáveis do _dataset_. No entanto, tendo em conta que a dimensionalidade dos _datasets_ em estudo não é muito elevada, o resto do estudo será feito sobre todas as variáveis visto que não é necessário descartar cerca de 30% de variabilidade quando a dimensionalidade não é um problema para estes _datasets_.


### Análise Discriminante

De modo a responder à última questão de interesse proposta para este _dataset_ recorreu-se à biblioteca `MASS` que disponibiliza a função `lda` que permite realizar o método da análise discriminante de uma maneira simples, para tal primeiro foi necessário trabalhar os dados de maneira a obter um novo _data-frame_ em que as regiões dos países fosse o continente em que se situam, este processamento encontra-se no ficheiro `world_happiness.R`

```{r making_continents, include=FALSE, echo=FALSE}
library(data.table) #Para usar a função %like%

#Obter as variáveis de interesse de países que a região contenha uma certa String
europa = dados_2017[dados_2017[, "Region"] %like% "Europe", -c(c(1:6), 13)]
america_norte = dados_2017[dados_2017[, "Region"] %like% "North America", -c(c(1:6), 13)]
america_sul = dados_2017[dados_2017[, "Region"] %like% "Latin America", -c(c(1:6), 13)]
africa = dados_2017[dados_2017[, "Region"] %like% "Africa", -c(c(1:6), 13)]
asia = dados_2017[dados_2017[, "Region"] %like% "Asia", -c(c(1:6), 13)]
oceania = dados_2017[dados_2017[, "Region"] %like% "Australia", -c(c(1:6), 13)]


#Adicionar o novo nome do grupo aos dados
europa$Region = "Europe"
america_norte$Region = "North America"
america_sul$Region = "South America"
africa$Region = "Africa"
asia$Region = "Asia"
oceania$Region = "Oceania"

#Juntar todos os dados num só data-frame
todos_grupos = rbind(
  rbind(
    rbind(
      rbind(
        rbind(europa, america_norte),
        america_sul),
      africa),
    asia),
  oceania)
```

Obtendo um _data-frame_ trabalhado é possível utilizar a função `lda` da biblioteca `MASS` e obter os coeficientes das combinações lineares

```{r lda,  echo = T, results = 'hide'}
library("MASS")

resultado_lda = lda(Region ~ GDP + Family + `Life Expectancy` + Freedom +
                      `Government Corruption` + Generosity, data = todos_grupos)

# Obtenção dos coeficientes das combinações lineares
resultado_lda$scaling
```

Tendo as funções que melhor discriminam os diferentes grupos do _dataset_ sabe-se como se destinguem os diferentes continentes. Com estas funções é interessante também implementar um classificador que consiga classificar países que não estejam agrupados a nenhum continente, caso por exemplo existisse algum erro na construção da tabela de dados dos países. Este processo de classificação é também possível de implementar através da função `predict` onde é passado o objeto que guarda a informação sobre as funções descriminantes (`resultado_lda`) e os dados que se pretende classificar.

```{r classify, echo = T, results = 'hide'}
#Construção do classificador
predictions = predict(resultado_lda, newdata = todos_grupos[, -7])

#Obtenção das previsões
predictions$class
```

É possível agora comparar as previsões feitas pelo classificador com a _ground truth_ inserindo ambos os dados numa tabela conhecida por _confusion matrix_.

```{r confusion_matrix, echo=T, results= 'hide'}
#Criação da confusion matrix com a ground truth e as previsões
table(todos_grupos[, 7], predictions$class)
```

\begin{table}[ht]
\centering
\label{tab:pca_result}
\begin{tabular}{ccccccc}
& Africa & Asia & Europe & North America & Oceania & South America \\ \hline
Africa & 47 & 2 & 9 & 0 & 0 & 0 \\ \hline
Asia & 4 & 8 & 5 & 0 & 1 & 3 \\ \hline
Europe & 3 & 3 & 41 & 0 & 0 & 3 \\ \hline
North America & 0 & 0 & 2 & 0 & 0 & 0 \\ \hline
Oceania & 0 & 0 & 1 & 0 & 1 & 0 \\ \hline
South America & 2 & 1 & 10 & 0 & 0 & 9 \\ \hline
\end{tabular}
\caption{\emph{Confusion Matrix} que relaciona a \emph{ground truth} com as previsões obtidas do classificador}
\end{table}


Da _confusion matrix_ é possível tirar informação sobre a taxa de acerto ($\frac{\# acertos}{\# exemplos}$) e a taxa de erro ($\frac{\# erros}{\# exemplos}$).

```{r accuracy, include=TRUE, echo=TRUE}
#A taxa de acerto é então a soma da diagonal sobre o número total de observações
taxa_acerto = sum(diag(table(todos_grupos[, 7], predictions$class))) / nrow(todos_grupos)

#E a taxa de erro é 1 - taxa de acerto
taxa_erro = 1 - taxa_acerto

cat("Taxa de acerto:",taxa_acerto,"\nTaxa de erro:",taxa_erro)
```

## Conclusões

Dado como concluido o estudo a estes três _datasets_ foi possível responder as perguntas __Serão os dados relativamente iguais entre os três anos?__ ; __Quais serão as variáveis que mais contribuem para a felicidade de um país?__ ; __Como se distinguem os diferentes continentes no que toca a sua felicidade?__ As respostas a estas perguntas foram obtidas utilizando diversas técnicas de análise estatistica aprendidas durante a unidade curricular de Estatistica Multivariada lecionada pela professora Regina Bispo.

Quanto a igualdade entres os três anos de medição do indice de felicidade dos países observou-se que as variáveis que o compões mantêm-se relativamente iguais. Neste estudo observou-se também que as variáveis que mais contribuem para a felicidade de um país são o seu PIB (produto interno bruto) e a contribuição da família. Por fim foi também estudado quanto à diferença entre os continentes no que toca a sua felicidade obtendo as funções que melhor os descriminam, foi também construido um pequeno classificador com base nestas funções que obteve uma taxa de acerto de 70% o que implica que com base nas variáveis e países existentes no _dataset_ não é possível classificar corretamente a que continente cada país pertence.

\newpage

## Referências